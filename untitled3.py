# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UDZnc9Ow7g-7h-czjCQIf1XQKovhxPDn
"""

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from google.colab import files
uploaded =files.upload()

!unzip '/content/archive.zip'

# Import necessary lib.

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Data augmentation on training variable

train_datagen = ImageDataGenerator(rescale=1./255,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

# Data augmentation on testing variable

test_datagen = ImageDataGenerator(rescale=1./255)

# Data augmentation on training data

xtest = test_datagen.flow_from_directory('/content/forest_fire/Testing',
                                           target_size=(64,64),
                                           class_mode='categorical',
                                           batch_size=100)

# Data augmentation on testing data

xtrain = train_datagen.flow_from_directory('/content/forest_fire/Training and Validation',
                                         target_size=(64,64),
                                         class_mode='categorical',
                                         batch_size=100)

"""CNN"""

# Importing req. lib.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense

# Build a CNN block

model = Sequential() # Initializing sequential model
model.add(Convolution2D(32,(3,3),activation='relu',input_shape=(64,64,3))) # convolution layer
model.add(MaxPooling2D(pool_size=(2, 2))) # Max pooling layer
model.add(Flatten()) # Flatten layer
model.add(Dense(300,activation='relu')) # Hidden layer 1
model.add(Dense(150,activation='relu')) # Hidden layer 2
model.add(Dense(2,activation='softmax')) # Output layer

# Compiling the model

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

# Train model

model.fit_generator(xtrain,
                    steps_per_epoch=len(xtrain),
                    epochs=20,
                    validation_data=xtest,
                    validation_steps=len(xtest))

# Save model

model.save('wildfire.h5')

"""TESTING"""

from tensorflow.keras.preprocessing import image
import numpy as np

# Testing 1

img = image.load_img('/content/forest_fire/Testing/fire/abc173.jpg',target_size=(64,64)) # Reading image
x = image.img_to_array(img) # Converting image into array
x = np.expand_dims(x,axis=0) # expanding Dimensions
pred = np.argmax(model.predict(x)) # Predicting the higher probablity index
op = ['fire','nofire'] # Creating list
op[pred] # List indexing with output

# Testing 2

img = image.load_img('/content/forest_fire/Testing/nofire/abc339.jpg',target_size=(64,64)) # Reading image
x = image.img_to_array(img) # Converting image into array
x = np.expand_dims(x,axis=0) # expanding Dimensions
pred = np.argmax(model.predict(x)) # Predicting the higher probablity index
op = ['fire','nofire'] # Creating list
op[pred] # List indexing with output

xtrain.class_indices

# Testing google image

img = image.load_img('/content/wildfire.jpg',target_size=(64,64)) # Reading image
x = image.img_to_array(img) # Converting image into array
x = np.expand_dims(x,axis=0) # expanding Dimensions
pred = np.argmax(model.predict(x)) # Predicting the higher probablity index
op = ['fire', 'nofire'] # Creating list
op[pred] # List indexing with output

img = image.load_img('/content/NOFIRE.jpg',target_size=(64,64)) # Reading image
x = image.img_to_array(img) # Converting image into array
x = np.expand_dims(x,axis=0) # expanding Dimensions
pred = np.argmax(model.predict(x)) # Predicting the higher probablity index
op = ['fire', 'nofire'] # Creating list
op[pred] # List indexing with output









